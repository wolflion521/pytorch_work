{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four steps for basic attention implementation\n",
    "* 1. Get annotations from the encoder, and calculate the dot with decode hidden layer weights, and we call the result as attention scores of each annotation.\n",
    "* 2. Attention scores implement a softmax.\n",
    "* 3. Applying the scores back on the annotations. Here I mean merge all the annotations into a single annotation(which is called attention context vector).\n",
    "* 4. Attention context vector as the input to forward the decoder network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get attention scores\n",
    "here we get the raw weights without softmax, and softmax is supposed to be next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35725873 0.48181425 0.57194089 0.14378044 0.91018454 0.8616368\n",
      "  0.2403402  0.15624053]\n",
      " [0.75988586 0.46862045 0.73872135 0.97825797 0.14442042 0.48436769\n",
      "  0.3760081  0.41845616]\n",
      " [0.01081299 0.4763636  0.06018193 0.34687396 0.79705128 0.16394434\n",
      "  0.79526035 0.60014   ]\n",
      " [0.70621226 0.32898188 0.88855188 0.53922361 0.81180168 0.71360738\n",
      "  0.38716865 0.52776821]\n",
      " [0.85937518 0.83952762 0.15761333 0.27247348 0.76686171 0.70686928\n",
      "  0.95818711 0.29222771]\n",
      " [0.10120066 0.68565891 0.84657414 0.08651012 0.60532183 0.92895172\n",
      "  0.05481677 0.95400112]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# \n",
    "encoder_annotations =np.random.random((6,8))\n",
    "# 6 annotations, each one has 8 element\n",
    "print(encoder_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05645867 0.53103287 0.12225119 0.74492275 0.10278749 0.67898544\n",
      "  0.8115809  0.58182404]]\n"
     ]
    }
   ],
   "source": [
    "decoder_hidden_state = np.random.random((1,8))\n",
    "print(decoder_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.41760977, 2.00314326, 1.70716355, 1.91413745, 2.22302526,\n",
       "        1.83027236]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have 6 annotations, we want our annotation_weights_raw with the shape of (1,6)\n",
    "# so the shape of two matrixs is (1,8) and (8,6)\n",
    "annotation_weights_raw = np.matmul(decoder_hidden_state,np.transpose(encoder_annotations))\n",
    "annotation_weights_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From annotation_weights_raw calculate annotation scores\n",
    "Here we just simply implement a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1050062 , 0.18858576, 0.14027056, 0.17252584, 0.23496459,\n",
       "        0.15864705]], dtype=float128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x = np.array(x, dtype=np.float128)\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum(axis=1) \n",
    "scores = softmax(annotation_weights_raw)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying the scores back on the annotations. \n",
    "Here I mean merge all the annotations into a single annotation(which is called attention context vector)\n",
    "annotation shape is (6,8)\n",
    "result we want is (1,8)\n",
    "our scores shape is (1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52215256 0.56858332 0.53244962 0.41901576 0.65088812 0.64139903\n",
      "  0.50833176 0.49056936]]\n"
     ]
    }
   ],
   "source": [
    "context_vector = np.matmul(scores,encoder_annotations)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concat the context vector and the decoder hidden state and forward the decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
