{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "### structure\n",
    "    3 layer conv and downsample          \n",
    "    3 layer transpose conv upsample\n",
    "    loss: Mean Square Error\n",
    "    optimizer: Adam\n",
    "### functions\n",
    "    import torch.nn as nn\n",
    "    3 layer conv and downsample\n",
    "        conv:  nn.Conv2d(inputlayerchannels,outputlayerchannels,kernelsize,padding)\n",
    "        downsample:nn.MaxPool2d(2,2)\n",
    "    3 layer transpose conv upsample: nn.ConvTranspose2d(inputlayerchannels,outputlayerchannels,kernelsize,stride=2)\n",
    "    activation function:import torch.nn.functional as F   F.relu   F.sigmoid\n",
    "    loss: Mean Square Error   nn.MSELoss(predicts,targets)\n",
    "    optimizer: Adam        torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    Dataloader:torch.utils.data.DataLoader\n",
    "    reshape:  certainTensor.view(batch_size,channel,height,width)\n",
    "    get the output value of a layer: outputofalayer.detach().numpy() can get the auto_grad values away only keep the value of layeroutput\n",
    "### how to organize code\n",
    "    Write a class(nn.Module) to contain the whole network sturcture:__init__ and forward need to rewrite. \n",
    "    Conv and Transpose_Conv should keep the weights, while pool and relu don't need to keep paras. And nn calls layers, Function calls funcs,layers have paras so they should be the variable of network class.\n",
    "    instanciate the network class.\n",
    "    Define Loss \n",
    "    Define opitimizer\n",
    "    Image transform to Tensor, and load use Dataloader\n",
    "    iterate dataloader get the input and target batch to TRAIN the model\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss.item()*images.size(0) get total batch loss\n",
    "        train_loss update(each epoch or each batch)\n",
    "    TEST the model: get some data from test set, eg. one batch data. output = model(test_data)get the output. And RESHAPE \n",
    "    some visualize: fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "    for noisy_imgs, row in zip([noisy_imgs, output], axes):\n",
    "        for img, ax in zip(noisy_imgs, row):\n",
    "            ax.imshow(np.squeeze(img), cmap='gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████▍   | 9379840/9912422 [00:10<00:00, 1064189.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                        | 0/28881 [00:00<?, ?it/s]\n",
      "32768it [00:00, 78411.27it/s]                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                      | 0/1648877 [00:00<?, ?it/s]\n",
      "  5%|███▍                                                                  | 81920/1648877 [00:00<00:02, 773251.47it/s]\n",
      " 10%|███████▏                                                             | 172032/1648877 [00:00<00:01, 794943.96it/s]\n",
      " 15%|██████████▋                                                          | 253952/1648877 [00:00<00:01, 755581.37it/s]\n",
      " 19%|█████████████▎                                                       | 319488/1648877 [00:00<00:01, 706253.92it/s]\n",
      " 25%|█████████████████▏                                                   | 409600/1648877 [00:00<00:01, 745994.47it/s]\n",
      " 31%|█████████████████████▌                                               | 516096/1648877 [00:01<00:01, 801256.83it/s]\n",
      " 41%|████████████████████████████▍                                        | 679936/1648877 [00:01<00:01, 944731.57it/s]\n",
      " 55%|█████████████████████████████████████▌                              | 909312/1648877 [00:01<00:00, 1138680.00it/s]\n",
      " 67%|████████████████████████████████████████████▉                      | 1105920/1648877 [00:01<00:00, 1300733.90it/s]\n",
      " 80%|█████████████████████████████████████████████████████▌             | 1318912/1648877 [00:01<00:00, 1466873.60it/s]\n",
      "1654784it [00:01, 1066127.20it/s]                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                         | 0/4542 [00:00<?, ?it/s]\n",
      "8192it [00:00, 21565.91it/s]                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9920512it [00:29, 1064189.91it/s]                                                                                      "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.MNIST(root='data',train=True,download=True,transform=transform)\n",
    "test_data = datasets.MNIST(root='data',train=False,download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Step 1: write a class of network\n",
    "\n",
    "Write a class(nn.Module) to contain the whole network sturcture:__init__ and forward need to rewrite.        \n",
    "Conv and Transpose_Conv should keep the weights, while pool and relu don't need to keep paras. And nn calls layers, Function calls funcs,layers have paras so they should be the variable of network class.        \n",
    "instanciate the network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoiser(\n",
      "  (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv_1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (t_conv_2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv_3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class  Denoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Denoiser,self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(1,32,3,padding = 1)\n",
    "        self.conv_2 = nn.Conv2d(32,16,3,padding = 1)\n",
    "        self.conv_3 = nn.Conv2d(16,8,3,padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.t_conv_1 = nn.ConvTranspose2d(8,8,3,stride = 2)\n",
    "        self.t_conv_2 = nn.ConvTranspose2d(8,16,2,stride = 2)\n",
    "        self.t_conv_3 = nn.ConvTranspose2d(16,32,2,stride = 2)\n",
    "        self.conv_out = nn.Conv2d(32,1,3,padding = 1)\n",
    "    def forward(self,x):\n",
    "        # encode\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = self.pool(x)\n",
    "        # decode\n",
    "        x = F.relu(self.t_conv_1(x))\n",
    "        x = F.relu(self.t_conv_2(x))\n",
    "        x = F.relu(self.t_conv_3(x))\n",
    "        # output should have a sigmoid applied\n",
    "        x = F.sigmoid(self.conv_out(x))\n",
    "        return x\n",
    "model = Denoiser()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 2: \n",
    "Define Loss      \n",
    "Define opitimizer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 3: \n",
    "Data transform to Tensor, and load data using DataLoader\n",
    "DataLoader contain Sampler, BatchSampler, and dataset, allowing us sample the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Create training and test dataloaders\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1387cfa9860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# how to get batches when training\n",
    "\n",
    "epochs = 2\n",
    "for i in range(epochs):\n",
    "    count = 0\n",
    "    for data in train_loader:\n",
    "        image,_ = data\n",
    "        print(image.shape)\n",
    "        count += 1\n",
    "        if count>5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 4: Train\n",
    "iterate dataloader get the input and target batch to TRAIN the model         \n",
    "    optimizer.zero_grad()        \n",
    "    outputs = model(inputs)        \n",
    "    loss = loss_function(outputs,targets)          \n",
    "    loss.backward()         \n",
    "    optimizer.step()         \n",
    "    loss.item()*images.size(0) get total batch loss        \n",
    "    train_loss update(each epoch or each batch)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "noise_factor = 0.5\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        images,_ = data\n",
    "        \n",
    "        noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(noisy_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
